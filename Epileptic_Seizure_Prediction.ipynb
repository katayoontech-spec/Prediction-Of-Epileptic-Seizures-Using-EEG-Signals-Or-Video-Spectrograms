{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGdvfbZ8aBO_"
      },
      "source": [
        "# In the name of God."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPoBIHVOaBkt"
      },
      "source": [
        "# Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WelD9lbIv68J"
      },
      "source": [
        "# Epileptic Seizure Prediction Using EEG Signals or Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTd4CbMbaEDx"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access EEG data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check the contents of the chb directory\n",
        "import os\n",
        "\n",
        "# Path\n",
        "chb_path = '/content/drive/MyDrive/G_Pr/chb'\n",
        "\n",
        "# List the folders (patients)\n",
        "print(\"Available patient folders:\")\n",
        "print(os.listdir(chb_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjLULEeEaEGc"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "# List .edf files for each patient\n",
        "for patient in os.listdir(chb_path):\n",
        "    patient_folder = os.path.join(chb_path, patient)\n",
        "    edf_files = sorted(glob.glob(os.path.join(patient_folder, \"*.edf\")))\n",
        "\n",
        "    print(f\"{patient}: {len(edf_files)} EDF files\")\n",
        "    for edf_file in edf_files[:3]:  # Just show first 3 as preview in each folder\n",
        "        print(\"   \", os.path.basename(edf_file))\n",
        "    print(\"   ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pch6ppJVo7fa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdQM6t1vaEI7"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install mne matplotlib numpy\n",
        "\n",
        "# --- Load one EDF file to inspect EEG signals ---\n",
        "\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to one sample EDF file (you can change the patient and file later)\n",
        "edf_sample_path = os.path.join(chb_path, \"chb01\", \"chb01_01.edf\")\n",
        "\n",
        "# Load the EDF file\n",
        "raw = mne.io.read_raw_edf(edf_sample_path, preload=True)\n",
        "print(raw)\n",
        "\n",
        "# Plot EEG signals (first 30 seconds)\n",
        "raw.plot(duration=30, n_channels=23, scalings='auto', show=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBeKcx78aELq"
      },
      "outputs": [],
      "source": [
        "# Load a specific EDF file and plot EEG signals\n",
        "\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to a specific EDF file (e.g., chb01_01.edf from subject chb01)\n",
        "edf_file = '/content/drive/MyDrive/G_Pr/chb/chb01/chb01_01.edf'\n",
        "\n",
        "# Load the EEG data from the EDF file with preload=True to load it into memory\n",
        "raw = mne.io.read_raw_edf(edf_file, preload=True)\n",
        "\n",
        "# Display basic information about the EEG recording\n",
        "print(raw.info)\n",
        "\n",
        "# Plot the first 10 seconds of EEG data using 20 channels and automatic scaling\n",
        "raw.plot(duration=10, n_channels=20, scalings='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npCP-pJPaEOP"
      },
      "outputs": [],
      "source": [
        "# Extract raw EEG data and timestamps\n",
        "\n",
        "# Get data and timestamps\n",
        "eeg_data, times = raw.get_data(return_times=True)\n",
        "\n",
        "print(\"EEG data shape:\", eeg_data.shape)   # (channels, samples)\n",
        "print(\"Timestamps shape:\", times.shape)    # (samples,)\n",
        "print(\"Sample rate:\", raw.info['sfreq'])   # sampling frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwkq7-U0aEUx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import mne\n",
        "\n",
        "# Root directory containing the EEG data\n",
        "root_dir = \"/content/drive/MyDrive/G_Pr/chb\"\n",
        "\n",
        "# Band-pass filter range (1 to 40 Hz)\n",
        "low_freq = 1.0\n",
        "high_freq = 40.0\n",
        "\n",
        "# Get the list of patient directories\n",
        "patients = sorted([p for p in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, p))])\n",
        "\n",
        "# Iterate through all patients and their EDF files\n",
        "for patient in patients:\n",
        "    patient_path = os.path.join(root_dir, patient)\n",
        "    edf_files = sorted([f for f in os.listdir(patient_path) if f.endswith('.edf')])\n",
        "\n",
        "    print(f\"\\n Patient {patient} - {len(edf_files)} EDF files\")\n",
        "\n",
        "    for edf_file in edf_files:\n",
        "        edf_path = os.path.join(patient_path, edf_file)\n",
        "        print(f\" Loading {edf_file}...\")\n",
        "\n",
        "        try:\n",
        "            # Load the raw EDF file\n",
        "            raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
        "\n",
        "            # Apply band-pass filter from 1 to 40 Hz\n",
        "            raw.filter(l_freq=low_freq, h_freq=high_freq, verbose=False)\n",
        "\n",
        "            # Extract the data and corresponding timestamps\n",
        "            data, timestamps = raw.get_data(return_times=True)\n",
        "            print(f\" Done. Data shape: {data.shape}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Failed to process {edf_file}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xDOan_xvWWr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "def parse_summary_file(summary_path):\n",
        "    seizure_dict = {}\n",
        "    current_edf = None\n",
        "\n",
        "    with open(summary_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            # Detect EDF file name\n",
        "            if line.endswith('.edf'):\n",
        "                current_edf = line\n",
        "                seizure_dict[current_edf] = []\n",
        "\n",
        "            # Detect seizure start/end\n",
        "            elif 'Seizure' in line and 'Start Time' in line:\n",
        "                start_time = float(re.findall(r'(\\d+\\.?\\d*)', line)[0])\n",
        "                seizure_dict[current_edf].append({'start': start_time})\n",
        "            elif 'Seizure' in line and 'End Time' in line:\n",
        "                end_time = float(re.findall(r'(\\d+\\.?\\d*)', line)[0])\n",
        "                if seizure_dict[current_edf]:\n",
        "                    seizure_dict[current_edf][-1]['end'] = end_time\n",
        "\n",
        "    return seizure_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QosU1pVpvWZw"
      },
      "outputs": [],
      "source": [
        "# --- Read seizure info for all patients from their summary files ---\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/G_Pr/chb\"\n",
        "patient_folders = sorted([f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))])\n",
        "\n",
        "all_seizure_data = {}\n",
        "\n",
        "for patient_id in patient_folders:\n",
        "    summary_path = os.path.join(base_dir, patient_id, f\"{patient_id}-summary.txt\")\n",
        "    if os.path.exists(summary_path):\n",
        "        print(f\"Parsing: {summary_path}\")\n",
        "        seizure_info = parse_summary_file(summary_path)\n",
        "        all_seizure_data[patient_id] = seizure_info\n",
        "    else:\n",
        "        print(f\"Summary file not found for {patient_id}\")\n",
        "\n",
        "print(\"\\n Finished parsing all patients.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD1jOH651nZ_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mne\n",
        "\n",
        "# Root directory\n",
        "root_dir = '/content/drive/MyDrive/G_Pr/chb'\n",
        "\n",
        "# Output list\n",
        "all_features = []\n",
        "expected_channels = None  # Number of channels to be used\n",
        "\n",
        "for patient_folder in os.listdir(root_dir):\n",
        "    patient_path = os.path.join(root_dir, patient_folder)\n",
        "    if not os.path.isdir(patient_path):\n",
        "        continue\n",
        "\n",
        "    seizures = all_seizure_data.get(patient_folder, [])\n",
        "\n",
        "    for edf_file in os.listdir(patient_path):\n",
        "        if not edf_file.endswith('.edf'):\n",
        "            continue\n",
        "\n",
        "        edf_path = os.path.join(patient_path, edf_file)\n",
        "\n",
        "        try:\n",
        "            raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
        "            data, times = raw.get_data(return_times=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {edf_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Set expected channel count from first successful file\n",
        "        if expected_channels is None:\n",
        "            expected_channels = data.shape[0]\n",
        "\n",
        "        # Skip files with different number of channels\n",
        "        if data.shape[0] != expected_channels:\n",
        "            print(f\"Skipping {edf_file} due to mismatched channel count.\")\n",
        "            continue\n",
        "\n",
        "        # Seizure detection\n",
        "        contains_seizure = 0\n",
        "        for item in seizures:\n",
        "            if not isinstance(item, (list, tuple)) or len(item) != 3:\n",
        "                print(f\" Warning: unexpected seizure entry format: {item}\")\n",
        "                continue\n",
        "            seizure_file, onset, duration = item\n",
        "            if seizure_file == edf_file:\n",
        "                contains_seizure = 1\n",
        "                break\n",
        "\n",
        "        # Extract features\n",
        "        features = []\n",
        "        for channel in data:\n",
        "            features.extend([\n",
        "                np.mean(channel),\n",
        "                np.std(channel),\n",
        "                np.sum(channel ** 2) / len(channel),\n",
        "            ])\n",
        "\n",
        "        features.append(contains_seizure)\n",
        "        features.append(patient_folder)\n",
        "        features.append(edf_file)\n",
        "\n",
        "        all_features.append(features)\n",
        "\n",
        "# Create column names\n",
        "column_names = []\n",
        "for ch in range(expected_channels):\n",
        "    column_names.extend([f'ch{ch+1}_mean', f'ch{ch+1}_std', f'ch{ch+1}_energy'])\n",
        "column_names += ['seizure', 'patient', 'filename']\n",
        "\n",
        "# Convert to DataFrame\n",
        "features_df = pd.DataFrame(all_features, columns=column_names)\n",
        "\n",
        "# Preview\n",
        "features_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-78hi1GQ_WZ"
      },
      "source": [
        "# Initial setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXZ__Q50GQ3I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import mne\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "\n",
        "# Parameters\n",
        "WINDOW_SIZE = 10  # in seconds\n",
        "SAMPLING_RATE = 256  # Sampling rate for CHB-MIT dataset\n",
        "WINDOW_SAMPLES = WINDOW_SIZE * SAMPLING_RATE  # Number of samples per segment\n",
        "\n",
        "EDF_DIR = '/content/chbmit'  # Path to EDF files\n",
        "ANNOTATION_FILE = 'chb-summary.csv'  # Seizure summary file (default)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE2nVTl5QyMU"
      },
      "source": [
        "# Function for extracting segments from files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xiwB0k5GQ5w"
      },
      "outputs": [],
      "source": [
        "def extract_segments(edf_file, seizure_starts, seizure_ends):\n",
        "    raw = mne.io.read_raw_edf(edf_file, preload=True, verbose=False)\n",
        "    data = raw.get_data()\n",
        "    num_samples = data.shape[1]\n",
        "    num_channels = data.shape[0]\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Sliding over the entire file with a fixed-size window\n",
        "    for start in range(0, num_samples - WINDOW_SAMPLES, WINDOW_SAMPLES):\n",
        "        end = start + WINDOW_SAMPLES\n",
        "        segment = data[:, start:end]\n",
        "\n",
        "        label = 0  # Default: no seizure\n",
        "        for sz_start, sz_end in zip(seizure_starts, seizure_ends):\n",
        "            sz_start_sample = int(sz_start * SAMPLING_RATE)\n",
        "            sz_end_sample = int(sz_end * SAMPLING_RATE)\n",
        "            if end > sz_start_sample - 256 and start < sz_start_sample:\n",
        "                label = 1  # Segment just before seizure onset\n",
        "                break\n",
        "\n",
        "        X.append(segment)\n",
        "        y.append(label)\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8I7FiyONtln"
      },
      "source": [
        "# General function for reading files and creating dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XF0xewZGQ9a"
      },
      "outputs": [],
      "source": [
        "def process_all_patients(base_dir, summary_csv):\n",
        "    summary_df = pd.read_csv(summary_csv)\n",
        "    all_X = []\n",
        "    all_y = []\n",
        "\n",
        "    for _, row in tqdm(summary_df.iterrows(), total=len(summary_df)):\n",
        "        patient = row['patient']\n",
        "        filename = row['filename']\n",
        "        seizures = eval(row['seizure_intervals'])\n",
        "\n",
        "        seizure_starts = [s[0] for s in seizures]\n",
        "        seizure_ends = [s[1] for s in seizures]\n",
        "\n",
        "        full_path = os.path.join(base_dir, patient, filename)\n",
        "        if not os.path.exists(full_path):\n",
        "            continue\n",
        "\n",
        "        X, y = extract_segments(full_path, seizure_starts, seizure_ends)\n",
        "        all_X.append(X)\n",
        "        all_y.append(y)\n",
        "\n",
        "    X = np.concatenate(all_X, axis=0)\n",
        "    y = np.concatenate(all_y, axis=0)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxrStS5HOlcU"
      },
      "source": [
        "# Creating csv summary file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iou2usI1GRQs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "SUMMARY_FILES = [\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb01/chb01-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb02/chb02-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb03/chb03-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb04/chb04-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb05/chb05-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb06/chb06-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb07/chb07-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb08/chb08-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb09/chb09-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb10/chb10-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb11/chb11-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb12/chb12-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb13/chb13-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb14/chb14-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb15/chb15-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb16/chb16-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb17/chb17-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb18/chb18-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb19/chb19-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb20/chb20-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb21/chb21-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb22/chb22-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb23/chb23-summary.txt\",\n",
        "    \"/content/drive/MyDrive/G_Pr/chb/chb24/chb24-summary.txt\"\n",
        "\n",
        "]\n",
        "\n",
        "summary_data = []\n",
        "\n",
        "for file_path in SUMMARY_FILES:\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    patient = os.path.basename(file_path).split('-')[0]\n",
        "    current_edf = None\n",
        "    seizure_times = []\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip().lower()\n",
        "\n",
        "        if \"file name:\" in line:\n",
        "            current_edf = line.split(\":\")[1].strip()\n",
        "            seizure_times = []\n",
        "\n",
        "        elif \"seizure start time\" in line:\n",
        "            # Extract number using regex\n",
        "            start_time = int(re.search(r'\\d+', line).group())\n",
        "            seizure_times.append([start_time, -1])\n",
        "\n",
        "        elif \"seizure end time\" in line and seizure_times:\n",
        "            end_time = int(re.search(r'\\d+', line).group())\n",
        "            seizure_times[-1][1] = end_time\n",
        "\n",
        "            summary_data.append({\n",
        "                \"patient\": patient,\n",
        "                \"file\": current_edf,\n",
        "                \"seizure_start\": seizure_times[-1][0],\n",
        "                \"seizure_end\": seizure_times[-1][1]\n",
        "            })\n",
        "\n",
        "df = pd.DataFrame(summary_data)\n",
        "df.to_csv(\"chb-summary.csv\", index=False)\n",
        "print(\" Summary file created: chb-summary.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4ULq6dHR_cy"
      },
      "outputs": [],
      "source": [
        "# +++\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mne\n",
        "from sklearn.utils import shuffle\n",
        "from joblib import dump\n",
        "\n",
        "EDF_DIR = \"/content/drive/MyDrive/G_Pr/chb\"\n",
        "SUMMARY_FILE = \"chb-summary.csv\"\n",
        "WINDOW_SIZE = 2560  # 10 seconds at 256 Hz\n",
        "STEP_SIZE = 2560    # non-overlapping windows\n",
        "\n",
        "def extract_features_from_edf(edf_path, seizure_intervals):\n",
        "    raw = mne.io.read_raw_edf(edf_path, preload=True, verbose=False)\n",
        "\n",
        "    raw_data = raw.get_data()\n",
        "    sfreq = int(raw.info['sfreq'])  # typically 256 Hz\n",
        "\n",
        "    seizure_data = []\n",
        "    non_seizure_data = []\n",
        "\n",
        "    n_channels, n_samples = raw_data.shape\n",
        "\n",
        "    for start in range(0, n_samples - WINDOW_SIZE, STEP_SIZE):\n",
        "        end = start + WINDOW_SIZE\n",
        "        window = raw_data[:, start:end]\n",
        "\n",
        "        window_start_sec = start // sfreq\n",
        "        window_end_sec = end // sfreq\n",
        "\n",
        "        label = 0  # non-seizure by default\n",
        "        for s_start, s_end in seizure_intervals:\n",
        "            if (window_start_sec < s_end and window_end_sec > s_start):\n",
        "                label = 1\n",
        "                break\n",
        "\n",
        "        if label == 1:\n",
        "            seizure_data.append(window)\n",
        "        else:\n",
        "            non_seizure_data.append(window)\n",
        "\n",
        "    # Balance classes\n",
        "    min_len = min(len(seizure_data), len(non_seizure_data))\n",
        "    seizure_data = seizure_data[:min_len]\n",
        "    non_seizure_data = non_seizure_data[:min_len]\n",
        "\n",
        "    X = np.array(seizure_data + non_seizure_data)\n",
        "    y = np.array([1]*len(seizure_data) + [0]*len(non_seizure_data))\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def build_dataset_from_summary(summary_file, edf_dir):\n",
        "    df = pd.read_csv(summary_file)\n",
        "    all_X = []\n",
        "    all_y = []\n",
        "\n",
        "    expected_shape = None\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        edf_path = os.path.join(edf_dir, row[\"patient\"], row[\"file\"])\n",
        "        if not os.path.exists(edf_path):\n",
        "            print(f\"File not found: {edf_path}\")\n",
        "            continue\n",
        "\n",
        "        if row[\"seizure_start\"] == -1:\n",
        "            seizure_intervals = []\n",
        "        else:\n",
        "            seizure_intervals = [(int(row[\"seizure_start\"]), int(row[\"seizure_end\"]))]\n",
        "\n",
        "        try:\n",
        "            X, y = extract_features_from_edf(edf_path, seizure_intervals)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {edf_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Only Keep files with same channels\n",
        "        if expected_shape is None:\n",
        "            expected_shape = X.shape[1:]\n",
        "        elif X.shape[1:] != expected_shape:\n",
        "            print(f\"Skipping due to shape mismatch: {edf_path}\")\n",
        "            continue\n",
        "\n",
        "        all_X.append(X)\n",
        "        all_y.append(y)\n",
        "\n",
        "    X = np.concatenate(all_X, axis=0)\n",
        "    y = np.concatenate(all_y, axis=0)\n",
        "    X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Run and save\n",
        "X, y = build_dataset_from_summary(SUMMARY_FILE, EDF_DIR)\n",
        "print(\"Final dataset shape:\", X.shape, y.shape)\n",
        "\n",
        "# Save using joblib\n",
        "dump((X, y), \"eeg_dataset.joblib\")\n",
        "print(\"Dataset saved as eeg_dataset.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W__DC3LIR_fi"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load the dataset\n",
        "X, y = joblib.load('/content/eeg_dataset.joblib')\n",
        "\n",
        "# Ensure the data type is float32 for compatibility with Keras models\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.int32)\n",
        "\n",
        "# Shuffle the data\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk_615glZD7E"
      },
      "source": [
        "# CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZZoqRNMM8yn"
      },
      "outputs": [],
      "source": [
        "print(\"Train class distribution:\", np.bincount(y_train))\n",
        "print(\"Test class distribution:\", np.bincount(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z49tAUc6kwa6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "\n",
        "# === Reshape EEG input to [samples, time, channels] ===\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[2], X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[2], X_test.shape[1]))\n",
        "\n",
        "# === Normalize using standardization ===\n",
        "mean = np.mean(X_train_reshaped, axis=(0, 1), keepdims=True)\n",
        "std = np.std(X_train_reshaped, axis=(0, 1), keepdims=True)\n",
        "X_train_norm = (X_train_reshaped - mean) / (std + 1e-7)\n",
        "X_test_norm = (X_test_reshaped - mean) / (std + 1e-7)\n",
        "\n",
        "# === Build optimized 1D-CNN model (simplified & regularized) ===\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train_norm.shape[1], X_train_norm.shape[2])),\n",
        "\n",
        "    Conv1D(16, kernel_size=7, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Conv1D(32, kernel_size=5, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dropout(0.6),\n",
        "\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# === Compile the model ===\n",
        "model.compile(optimizer=Adam(learning_rate=0.0003),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# === Callbacks for improved training ===\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "# Save the best model during training\n",
        "checkpoint = ModelCheckpoint(\"best_model.keras\", monitor='val_loss',\n",
        "                             save_best_only=True, mode='min', verbose=1)\n",
        "\n",
        "# === Train the model ===\n",
        "history = model.fit(X_train_norm, y_train,\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.25,\n",
        "                    verbose=1,\n",
        "                    callbacks=[early_stop, reduce_lr, checkpoint])\n",
        "\n",
        "# === Load best weights if needed (optional after training) ===\n",
        "model.load_weights(\"best_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0oCBuxaOrv_"
      },
      "outputs": [],
      "source": [
        "# === Predict probabilities (sigmoid output) ===\n",
        "y_pred = model.predict(X_test_norm).ravel()  # Probabilities (values between 0 and 1)\n",
        "\n",
        "# === Convert probabilities to binary class predictions ===\n",
        "y_pred_binary = (y_pred >= 0.5).astype(int)\n",
        "\n",
        "# === Evaluation Metrics ===\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_binary))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred))  # Not y_pred_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAS_qt6DRpku"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyRFY5E88_85"
      },
      "outputs": [],
      "source": [
        "!pip install mne --quiet\n",
        "!pip install matplotlib numpy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PgZfDJoiLbQ"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKgOslILiLeD"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access EEG data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check the contents of the chb directory\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f5GQ6NhiLgw"
      },
      "outputs": [],
      "source": [
        "# Path to the EEG file with a seizure\n",
        "seizure_file = \"/content/drive/MyDrive/G_Pr/chb/chb01/chb01_15.edf\"\n",
        "\n",
        "# Path to the normal EEG file\n",
        "normal_file = \"/content/drive/MyDrive/G_Pr/chb/chb01/chb01_09.edf\"\n",
        "\n",
        "# Load the EEG file with a seizure\n",
        "raw_seizure = mne.io.read_raw_edf(seizure_file, preload=True)\n",
        "\n",
        "# Load the normal EEG file\n",
        "raw_normal = mne.io.read_raw_edf(normal_file, preload=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_U3u2wOjMuV"
      },
      "outputs": [],
      "source": [
        "# Display information about the seizure EEG file\n",
        "raw_seizure.info\n",
        "\n",
        "# Plot the first few seconds of the seizure EEG file for inspection\n",
        "raw_seizure.plot(n_channels=10, duration=10, scalings='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkRBG2GhjMw5"
      },
      "outputs": [],
      "source": [
        "def segment_raw_data(raw, segment_duration_sec=5):\n",
        "    \"\"\"\n",
        "    Segments raw EEG data into fixed-duration windows.\n",
        "\n",
        "    Parameters:\n",
        "        raw (mne.io.Raw): Raw EEG data\n",
        "        segment_duration_sec (int): Duration of each segment in seconds\n",
        "\n",
        "    Returns:\n",
        "        List of numpy arrays, each representing a segment\n",
        "    \"\"\"\n",
        "    sampling_rate = int(raw.info['sfreq'])\n",
        "    segment_samples = segment_duration_sec * sampling_rate\n",
        "    data, _ = raw[:]\n",
        "    num_segments = data.shape[1] // segment_samples\n",
        "    segments = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        start = i * segment_samples\n",
        "        end = start + segment_samples\n",
        "        segment = data[:, start:end]\n",
        "        if segment.shape[1] == segment_samples:\n",
        "            segments.append(segment)\n",
        "\n",
        "    return segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl0ZDqNwjMzs"
      },
      "outputs": [],
      "source": [
        "# Segment both seizure and normal data into 5-second chunks\n",
        "seizure_segments = segment_raw_data(raw_seizure, segment_duration_sec=5)\n",
        "normal_segments = segment_raw_data(raw_normal, segment_duration_sec=5)\n",
        "\n",
        "print(f\"Number of seizure segments: {len(seizure_segments)}\")\n",
        "print(f\"Number of normal segments: {len(normal_segments)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS5WUb1hjM2U"
      },
      "outputs": [],
      "source": [
        "import mne\n",
        "\n",
        "# Load the EDF file (you can change to your own path)\n",
        "raw = mne.io.read_raw_edf(\"/content/drive/MyDrive/G_Pr/chb/chb01/chb01_15.edf\", preload=True)\n",
        "\n",
        "# Print the list of EEG channel names\n",
        "print(\"Channel names in this EDF file:\")\n",
        "print(raw.ch_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75ekZ3Tpjyfi"
      },
      "source": [
        "# Creating Spectogram image from segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe7G4ScujM5G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import spectrogram\n",
        "from tqdm import tqdm\n",
        "\n",
        "def save_multi_channel_spectrograms(segments, raw_info, label, output_dir):\n",
        "    selected_channels = ['FP1-F3', 'F7-T7', 'T7-P7']\n",
        "    channel_indices = [raw_info['ch_names'].index(ch) for ch in selected_channels if ch in raw_info['ch_names']]\n",
        "\n",
        "    label_dir = os.path.join(output_dir, label)\n",
        "    os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "    for i, segment in tqdm(enumerate(segments), total=len(segments)):\n",
        "        selected_signals = [segment[idx] for idx in channel_indices]\n",
        "        combined_signal = np.mean(selected_signals, axis=0)\n",
        "\n",
        "        f, t, Sxx = spectrogram(combined_signal, fs=256)\n",
        "\n",
        "        plt.figure(figsize=(2.24, 2.24))\n",
        "        plt.pcolormesh(t, f, 10 * np.log10(Sxx), shading='gouraud')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout(pad=0)\n",
        "        filename = os.path.join(label_dir, f'{label}_{i}.png')\n",
        "        plt.savefig(filename, dpi=100, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5Ip_CDsjM8i"
      },
      "outputs": [],
      "source": [
        "save_multi_channel_spectrograms(seizure_segments, raw.info, label='seizure', output_dir='/content/eeg_spectrograms')\n",
        "save_multi_channel_spectrograms(normal_segments, raw.info, label='normal', output_dir='/content/eeg_spectrograms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCLB7u97iLjX"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj0-zoGOiLmt"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "from natsort import natsorted  # For numerical sorting of files\n",
        "\n",
        "# Path to the folder where spectrogram images are stored\n",
        "image_folder = '/content/eeg_spectrograms/normal/'\n",
        "\n",
        "# Output video file name\n",
        "video_name = 'eeg_spectrogram_video.avi'\n",
        "\n",
        "# Get the list of images and sort them numerically\n",
        "images = natsorted([img for img in os.listdir(image_folder) if img.endswith(\".png\")])\n",
        "\n",
        "# Read the dimensions of the first image to set the video size\n",
        "first_frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
        "height, width, _ = first_frame.shape\n",
        "\n",
        "# Define the output video using OpenCV\n",
        "video_writer = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'XVID'), 5, (width, height))\n",
        "\n",
        "# Add images to the video\n",
        "for image in images:\n",
        "    img_path = os.path.join(image_folder, image)\n",
        "    frame = cv2.imread(img_path)\n",
        "    video_writer.write(frame)\n",
        "\n",
        "video_writer.release()\n",
        "print(\" Video successfully created:\", video_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEEIwZH88__P"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture('eeg_spectrogram_video.avi')\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    cv2_imshow(frame)\n",
        "    # Pause to view the frame - waitKey doesn't work in Colab, so we use sleep instead\n",
        "    import time\n",
        "    time.sleep(0.5)  # 0.5 seconds between each frame\n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2RzGHlK9ACI"
      },
      "outputs": [],
      "source": [
        "# Convert video from AVI to MP4 using ffmpeg\n",
        "!ffmpeg -i eeg_spectrogram_video.avi -vcodec libx264 eeg_spectrogram_video.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1GC-vUC9AFg"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4_file = \"eeg_spectrogram_video.mp4\"\n",
        "\n",
        "# Read the video and convert it to HTML for playback inside Colab\n",
        "mp4 = open(mp4_file, 'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=600 controls>\n",
        "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8AzaCNrk39w"
      },
      "source": [
        "# Creating Dataset and Training CNN with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqgTCE8tkqQ_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6iBSAohlH_Y"
      },
      "source": [
        "# Definition of Customized EEG dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HRmwWxckqTZ"
      },
      "outputs": [],
      "source": [
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, image_dir, label, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label = label\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXjJmJHSlLZd"
      },
      "outputs": [],
      "source": [
        "# Transformations (resize to 224x224 and normalize for ResNet)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "seizure_dataset = EEGDataset('/content/eeg_spectrograms/seizure', label=1, transform=transform)\n",
        "normal_dataset = EEGDataset('/content/eeg_spectrograms/normal', label=0, transform=transform)\n",
        "\n",
        "# Combine and split\n",
        "full_dataset = seizure_dataset + normal_dataset\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKRS8CbQlbSV"
      },
      "source": [
        "# Use from Pretrained ResNet18 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M22f9tohlLb8"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "base_model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all layers except the last few (Optional)\n",
        "for param in base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the fully connected layer with one including dropout\n",
        "base_model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),              # Dropout 50%\n",
        "    nn.Linear(base_model.fc.in_features, 2)\n",
        ")\n",
        "\n",
        "model = base_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRc8VawwlLeV"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF73RQNwlLhz"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "num_epochs = 50  # Increase the number of epochs since we have EarlyStopping\n",
        "\n",
        "# Set up Early Stopping\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Add weight decay to optimizer (L2 Regularization)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Scheduler to reduce LR when validation loss stops improving\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ----------- Training ----------\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # ----------- Validation ----------\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% \"\n",
        "          f\"| Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # -------- Early Stopping --------\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(\" Early stopping activated.\")\n",
        "            break\n",
        "\n",
        "# Load the best weights\n",
        "model.load_state_dict(best_model_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZSHCyB4kqWw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loss\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Accuracy\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}